# AutoGen Planner + Assistant on Gemini (Minimal Streamlit UI) 🤖

A minimal, learning-focused multi‑agent workflow: a Planner proposes steps; an Assistant writes and executes Python via a User Proxy — all running on Gemini and wrapped in a clean Streamlit UI.


## Why This Matters

- 🧠 Two‑agent reasoning: A clear separation of planning vs. execution.
- 🔧 Real code execution: Assistant’s Python blocks run locally (via User Proxy).
- 🧪 Learning-first: Simple two‑step flow without legacy function-calling.
- 🔒 [.env] controls everything.


## Key Features

- **Planner + Assistant**: Planner suggests steps; Assistant implements them in code.
- **Two-Step Flow**: Get plan first → pass plan to Assistant as context.
- **Local Execution**: Code runs in `planning/` workdir via the User Proxy.
- **Minimal UI**: One text area and “Run Multi-Agent Chat” button.
- **.env Support**: Configure Gemini and runtime settings via environment variables.


## Installation and Setup

### Prerequisites
- Python 3.8 or later
- [uv](https://docs.astral.sh/uv/) (recommended) or pip
- Gemini API key
- Optional: Docker Desktop (if you want to run generated code in Docker)

### 1) Create .env with Gemini

Create a [.env] at the project root:

```bash
GEMINI_API_KEY=your_gemini_api_key_here
# Optional overrides:
GEMINI_MODEL=gemini-2.0-flash
GEMINI_BASE_URL=[https://generativelanguage.googleapis.com/v1beta/openai](https://generativelanguage.googleapis.com/v1beta/openai)

# Runtime options (optional)
TEMPERATURE=0
MAX_AUTO_REPLIES=10
USE_DOCKER=false

Tip: An .env.example is included; copy it to .env and fill your values.

### 2) Install Dependencies
Using uv (recommended):
```bash
uv sync
```

### 3) Run the App
```bash
uv run streamlit run streamlit_app.py
```

# App will be available at http://localhost:8501

### Project Structure
autogen-agents/
├── streamlit_app.py         # Minimal Streamlit app (Planner → Assistant two-step)
├── planning/                # Work dir for code generated by the Assistant
│   ├── find_repo.py         # Example artifacts created during runs
│   └── fix_typo.py
├── .env                     # Environment variables (create this)
├── .env.example             # Template for .env
├── .streamlit/
│   └── config.toml          # Dark theme for clean screenshots
├── pyproject.toml           # Dependencies (streamlit, autogen-agentchat, etc.)

### Usage
Start the app.
Enter a task (e.g., “Suggest a fix to an open good first issue of flaml”).
Click “Run Multi‑Agent Chat.”
Watch the “Conversation Log”:
Planner proposes a plan first.
Assistant then writes/executes Python code (with retries if needed).
See the final message and review any generated files under planning/.

### Troubleshooting
FLAML warning:
“flaml.automl is not available…” is harmless for this app (we’re not using AutoML). You can ignore it.
No response / no logs:
Ensure 
.env
 is loaded and GEMINI_API_KEY is set. Restart the app after changes.

Docker execution:
Set USE_DOCKER=true, have Docker Desktop running, and keep the docker Python package installed.
Environment Variables
GEMINI_API_KEY (required): Your Gemini key.
GEMINI_MODEL (optional): e.g., gemini-2.0-flash or gemini-1.5-pro.
GEMINI_BASE_URL (optional): Defaults to Google’s OpenAI-compatible endpoint.
TEMPERATURE (optional): Default 0.
MAX_AUTO_REPLIES (optional): Default 10.
USE_DOCKER (optional): true or false (default false).

### Contribution
Contributions are welcome. Please fork and open a PR with focused, learning‑oriented improvements (e.g., small UI clarifications, better prompts, or optional tests).